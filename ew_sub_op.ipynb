{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cql/anaconda2/lib/python2.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/Users/cql/anaconda2/lib/python2.7/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.cross_validation import cross_val_score # K折交叉验证模块\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import preprocessing\n",
    "from sklearn import svm\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "from sklearn import svm\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.multiclass import OutputCodeClassifier\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import f_regression\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from imblearn.pipeline import Pipeline as ImPineline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.grid_search import RandomizedSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from scipy.signal import lfilter\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "import time\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spec = total_train_feature.iloc[:,:-1].head()\n",
    "\n",
    "def specLineNum(spec, windowSize=2, threshold=0.8):\n",
    "    fs = pd.Series()\n",
    "    ss = pd.Series()\n",
    "    for i in range(windowSize, spec.shape[1]):\n",
    "        s = spec.iloc[:,i-windowSize]\n",
    "        e = spec.iloc[:,i]\n",
    "        isFs = (e>threshold) & ((e-s)>threshold)\n",
    "        isSS = (e<threshold) & ((e-s)<-threshold)\n",
    "        if(fs.size==0):\n",
    "            fs = isFs.astype(int)\n",
    "        else:\n",
    "            fs = fs + isFs.astype(int)\n",
    "        if(ss.size==0):\n",
    "            ss = isSS.astype(int)\n",
    "        else:\n",
    "            ss = ss + isSS.astype(int)\n",
    "    combine = pd.concat([fs, ss], axis=1)\n",
    "    combine.columns = ['fs','ss']\n",
    "    return combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qsoFeature(data):\n",
    "    std1 = data.iloc[:,850:880].std(axis=1)\n",
    "    std2 = data.iloc[:,885:898].std(axis=1)\n",
    "    std3 = data.iloc[:,900:1000].std(axis=1)\n",
    "    f1 = (std1<0.3) & (std2>0.35) & (std3<0.35)\n",
    "    f3 = (std2*std2)/(std1*std3+0.1)\n",
    "    d1 = std1+0.2\n",
    "    f2 = (std2>d1)\n",
    "    qsof = pd.concat([f1, f2, f3], axis=1)\n",
    "    qsof.columns = ['qso_f1','qso_f2','qso_f3']\n",
    "    return (qsof)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intervalEWFeature(d, start=0, end=-1, filterThrehold=1):\n",
    "    cp = d\n",
    "#     cp[abs(cp)<filterThrehold] = 0\n",
    "    rd = cp.iloc[:, start:end]\n",
    "    specLines = specLineNum(rd[abs(rd)>filterThrehold])\n",
    "    ew_absorb = rd[(rd<0)&(abs(rd)>filterThrehold)].sum(axis=1)\n",
    "    ew_fs = rd[(rd>0)&(abs(rd)>filterThrehold)].sum(axis=1)\n",
    "    return pd.concat([ew_absorb, ew_fs, specLines], axis=1)\n",
    "\n",
    "def smooth(filterSet):\n",
    "    n = 5  # the larger n is, the smoother curve will be\n",
    "    b = [1.0 / n] * n\n",
    "    a = 1\n",
    "    filterSet = lfilter(b, a, filterSet, axis=1)\n",
    "    return pd.DataFrame(filterSet)\n",
    "\n",
    "def EWByRangeList(d, rangeList, filterThrehold=1, isSmooth=False, doScale=True):\n",
    "    if doScale:\n",
    "        d = pd.DataFrame(scale(d, axis=1))\n",
    "    if isSmooth:\n",
    "        d=smooth(d)\n",
    "    ewfAll = pd.DataFrame()\n",
    "    for r in rangeList:\n",
    "        start, end = r\n",
    "        ewf = intervalEWFeature(d, start, end, filterThrehold)\n",
    "        newNames = ['ew_' + str(start) + '_' + str(end) + '_' + str(v) for v in ewf.columns.values]\n",
    "        ewf.columns = newNames\n",
    "        ewfAll = pd.concat([ewfAll,ewf], axis=1)\n",
    "    return ewfAll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def submit(fileName, testTotal, preds):\n",
    "    submission = testTotal[['id']]\n",
    "    submission['pred'] = preds\n",
    "    submission.to_csv('./predicts/'+fileName, header=False, index=False)\n",
    "    return submission\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractFeature(train_feature, partition=2, randomPartion=False, prefix=\"p\", onlyAvg=False, filterLow=False, levelFeature=False, scaleSpec=False, smooth=False):\n",
    "    if scaleSpec:\n",
    "        train_feature = pd.DataFrame(scale(train_feature, axis=1))\n",
    "    if smooth:\n",
    "        train_feature = train_feature.rolling(100, axis=1).mean()\n",
    "    if filterLow:\n",
    "        train_feature[train_feature<1] = 0\n",
    "    plen = train_feature.shape[1]/partition\n",
    "    features = pd.DataFrame()\n",
    "    avgs = pd.DataFrame()\n",
    "    for i in range(0, partition):\n",
    "        pstart = random.randint(0, train_feature.shape[1]-plen-1)\n",
    "        pendExclue = pstart + plen + 1\n",
    "        avgC = train_feature.iloc[:, pstart:pendExclue].mean(axis=1)\n",
    "        stdC = train_feature.iloc[:, pstart:pendExclue].std(axis=1)\n",
    "        maxC = train_feature.iloc[:, pstart:pendExclue].max(axis=1)\n",
    "        minC = train_feature.iloc[:, pstart:pendExclue].min(axis=1)\n",
    "        medianC = train_feature.iloc[:, pstart:pendExclue].median(axis=1)\n",
    "        diffC = train_feature.iloc[:, pstart:pendExclue].diff(axis=1).iloc[:,1:]\n",
    "        features[prefix+'_avg'+str(i)] = avgC\n",
    "        avgs[prefix+'_avg'+str(i)] = avgC\n",
    "        if onlyAvg==False:\n",
    "            features[prefix+'_std'+str(i)] = stdC\n",
    "            features[prefix+'_median'+str(i)] = medianC\n",
    "            features[prefix+'_max'+str(i)] = maxC\n",
    "            features[prefix+'_min'+str(i)] = minC\n",
    "            features[prefix+'_avg_m_std'+str(i)] = stdC/avgC\n",
    "            features[prefix+'_max_m_avg'+str(i)] = maxC/avgC\n",
    "            features[prefix+'_min_m_avg'+str(i)] = minC/avgC\n",
    "#             features[prefix+'_media_m_avg'+str(i)] = medianC/avgC\n",
    "    if levelFeature:\n",
    "        level_feature = extractFeature(avgs, prefix=\"l\", partition = 4)\n",
    "        features = pd.concat([features, level_feature], axis = 1)\n",
    "    return features.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readTotalFeature(fileName, maxFileIndex, hasLabel=True, partition = 20, ewFeature=True, addQsoFeature=True):\n",
    "    total_train_feature = pd.DataFrame()\n",
    "    for i in range(0, maxFileIndex+1):\n",
    "        print('processing:' + str(i))\n",
    "        train_set = pd.read_csv(fileName + str(i) +'.csv', header=None)\n",
    "        renameCol = {0:'id'}\n",
    "        if hasLabel:\n",
    "            renameCol[train_set.shape[1]-1] = 'label'\n",
    "        train_set.rename(columns=renameCol, inplace=True)\n",
    "        if hasLabel:\n",
    "            processed_train = pd.DataFrame()\n",
    "            rawFeature = train_set.iloc[:,1:-1]\n",
    "        else:\n",
    "            rawFeature = train_set.iloc[:,1:]\n",
    "        processed_train = extractFeature(rawFeature, partition, levelFeature=False, smooth=False)\n",
    "        if ewFeature:\n",
    "            rangeList = [(0,300), (0, 500), (500, 1000),(850, 1000), (1000, 1300),(1500,2000), (1500, 2600), (2500, 2600)]\n",
    "            ew_feature = EWByRangeList(rawFeature, rangeList, filterThrehold=0.5, doScale=True)\n",
    "            processed_train = pd.concat([processed_train, ew_feature], axis=1)\n",
    "        if addQsoFeature:\n",
    "            qso_feature = qsoFeature(rawFeature)\n",
    "            processed_train = pd.concat([processed_train, qso_feature], axis=1)\n",
    "        if hasLabel:\n",
    "            processed_train['label'] = train_set.label\n",
    "        else:\n",
    "            processed_train['id'] = train_set.id\n",
    "        total_train_feature = total_train_feature.append(processed_train, ignore_index=True)\n",
    "    return total_train_feature.sample(total_train_feature.shape[0], replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "validMode = True\n",
    "reFeature = True\n",
    "partition = 20\n",
    "rng = np.random.RandomState(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cql/anaconda2/lib/python2.7/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
      "  warnings.warn(\"Numerical issues were encountered \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing:1\n",
      "processing:2\n",
      "processing:3\n",
      "processing:4\n",
      "processing:5\n",
      "processing:6\n",
      "processing:7\n",
      "processing:8\n",
      "processing:9\n",
      "processing:10\n",
      "processing:11\n",
      "processing:12\n",
      "processing:13\n",
      "processing:14\n",
      "processing:15\n",
      "processing:16\n",
      "processing:17\n",
      "processing:18\n",
      "processing:19\n",
      "processing:20\n",
      "processing:21\n",
      "processing:22\n",
      "processing:23\n",
      "processing:24\n",
      "processing:25\n",
      "processing:26\n",
      "processing:27\n",
      "processing:28\n",
      "processing:29\n",
      "processing:30\n",
      "processing:31\n",
      "processing:32\n",
      "processing:33\n",
      "processing:34\n",
      "processing:35\n",
      "processing:36\n",
      "processing:37\n",
      "processing:38\n",
      "processing:39\n",
      "processing:40\n",
      "processing:41\n",
      "processing:42\n",
      "processing:43\n",
      "processing:44\n",
      "processing:45\n",
      "processing:46\n",
      "processing:47\n",
      "processing:48\n"
     ]
    }
   ],
   "source": [
    "if reFeature:\n",
    "    total_train_feature = readTotalFeature('./train_feature_all_', 48, partition=partition)\n",
    "    total_train_feature.to_csv('./train_feature_check_point.csv', index=False)\n",
    "else :\n",
    "    total_train_feature = pd.read_csv('./train_feature_check_point.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_train_feature = total_train_feature.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34288, 196)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_train_feature[total_train_feature.label=='unknown'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(338695, 195)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = total_train_feature.iloc[:,:-1]\n",
    "Y = total_train_feature.iloc[:,-1]\n",
    "\n",
    "# Split the dataset in two equal parts\n",
    "x, t_x, y, t_y = train_test_split(\n",
    "    X, Y, test_size=0.3, random_state=0)\n",
    "x.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outlier_rejection(X, y):\n",
    "    model = IsolationForest(n_jobs=-1, random_state=rng)\n",
    "    model.fit(X)\n",
    "    y_pred = model.predict(X)\n",
    "    return X[y_pred == 1], y[y_pred == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote = SMOTE()\n",
    "anova_filter = SelectKBest(f_classif, k=(int)(x.shape[1]*0.9))\n",
    "model = RandomForestClassifier(n_jobs=8, class_weight='balanced', max_depth=60, n_estimators=100,  verbose=1,random_state=rng)\n",
    "# model = AdaBoostClassifier(random_state=rng)\n",
    "# model = GradientBoostingClassifier(random_state=rng,verbose=1)\n",
    "# model = XGBClassifier()\n",
    "# model = KNeighborsClassifier(n_neighbors = 3, n_jobs=-1)\n",
    "scaler = StandardScaler()\n",
    "clf = ImPineline([\n",
    "    ('anova', anova_filter),\n",
    "    ('scaler',scaler),\n",
    "    ('smote',smote),\n",
    "    ('model', model)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if validMode:\n",
    "#     rs = clf.fit(x, y)\n",
    "    scores = cross_val_score(clf, x, y, cv=3, n_jobs=-1, verbose=1, scoring='f1_macro')\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "#     preds_test = clf.predict(t_x)\n",
    "    # clf.score(t_x, t_y)\n",
    "#     print(\"%0.3f\" % f1_score(t_y, preds_test, average='macro'))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if validMode:\n",
    "    rs = clf.fit(x, y)\n",
    "    preds_test = clf.predict(t_x)\n",
    "#     preds_test = carbateQso(t_x, preds_test)\n",
    "#     clf.score(t_x, t_y)\n",
    "    print(\"%0.3f\" % f1_score(t_y, preds_test, average='macro'))  \n",
    "    print(classification_report(t_y, preds_test, labels=['galaxy', 'qso', 'star', 'unknown']))\n",
    "    print(confusion_matrix(t_y, preds_test, labels=['galaxy', 'qso', 'star', 'unknown']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if validMode:\n",
    "    clf = RandomForestClassifier(n_jobs=-1, class_weight='balanced',  verbose=1 )\n",
    "    rs = clf.fit(x, y)\n",
    "    preds_test = clf.predict(t_x)\n",
    "    # clf.score(t_x, t_y)\n",
    "    print('f1: %.2f' % f1_score(t_y, preds_test, average='macro')) \n",
    "    print(classification_report(t_y, preds_test, labels=['galaxy', 'qso', 'star', 'unknown']))\n",
    "    print(confusion_matrix(t_y, preds_test, labels=['galaxy', 'qso', 'star', 'unknown']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if validMode:\n",
    "    importances = model.feature_importances_\n",
    "    std = np.std([tree.feature_importances_ for tree in model.estimators_],\n",
    "                 axis=0)\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "\n",
    "    # Print the feature ranking\n",
    "    print(\"Feature ranking:\")\n",
    "\n",
    "    for f in range(X.shape[1]):\n",
    "        print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n",
    "\n",
    "    # Plot the feature importances of the forest\n",
    "    plt.figure()\n",
    "    plt.title(\"Feature importances\")\n",
    "    plt.bar(range(X.shape[1]), importances[indices],\n",
    "           color=\"r\", yerr=std[indices], align=\"center\")\n",
    "    plt.xticks(range(X.shape[1]), indices)\n",
    "    plt.xlim([-1, X.shape[1]])\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if validMode:\n",
    "    smote = SMOTE()\n",
    "    parameters = {\n",
    "        'max_depth':[60, 70, 80],  #70\n",
    "        'max_features':[0.5,0.6,0.7], #0.6\n",
    "    #     'min_samples_split':[2, 4, 8, 20], #2\n",
    "    #     'min_samples_leaf':[1,4,8,20], #1\n",
    "    #     'min_weight_fraction_leaf':[0,0.05,0.1,0.2], #0\n",
    "        'max_leaf_nodes':[800, 1000, 1200], #1000\n",
    "    #      'bootstrap':[True, False], #False\n",
    "    #     'oob_score':[True, False], #False\n",
    "    #     'class_weight':[None,'balanced'], #balanced\n",
    "        'n_estimators':[10, 20], #10\n",
    "    #     'criterion':['gini','entropy']\n",
    "    }\n",
    "    clf = GridSearchCV(RandomForestClassifier(n_jobs=4, max_depth=20), parameters, cv=5,\n",
    "                           scoring='f1_macro', n_jobs=8, verbose=2)\n",
    "    clf.fit(x, y)\n",
    "    print(clf.best_params_)\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing:0\n",
      "processing:1\n",
      "processing:2\n",
      "processing:3\n",
      "processing:4\n",
      "processing:5\n",
      "processing:6\n",
      "processing:7\n",
      "processing:8\n",
      "processing:9\n"
     ]
    }
   ],
   "source": [
    "if reFeature:\n",
    "    testTotal = readTotalFeature('./test_feature_all_', 9, hasLabel=False, partition=partition)\n",
    "    testTotal.to_csv('./test_feature_checkpoint.csv', index=False)\n",
    "    testTotal.head()\n",
    "else :\n",
    "    testTotal = pd.read_csv('./test_feature_checkpoint.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 196)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testTotal.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:  8.5min\n",
      "[Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed: 21.8min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('anova', SelectKBest(k=175, score_func=<function f_classif at 0x1a162525f0>)), ('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('smote', SMOTE(k=None, k_neighbors=5, kind='regular', m=None, m_neighbors=10, n_jobs=1,\n",
       "   out_step=0.5, random_state=None, ratio='auto', svm_... random_state=<mtrand.RandomState object at 0x1106e2910>,\n",
       "            verbose=1, warm_start=False))])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X, Y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed:    0.8s finished\n",
      "/Users/cql/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "test_x = testTotal.iloc[:,:-1]\n",
    "pred = clf.predict(test_x)\n",
    "timestr = time.strftime(\"%m%d_%H%M\")\n",
    "rs = submit(timestr+'_rf_ew_p_20.csv',testTotal, pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs.pred.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_train_feature.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_train_feature[:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
