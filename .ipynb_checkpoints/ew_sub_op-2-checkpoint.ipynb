{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.cross_validation import cross_val_score # K折交叉验证模块\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import preprocessing\n",
    "from sklearn import svm\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "from sklearn import svm\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.multiclass import OutputCodeClassifier\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import f_regression\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from imblearn.pipeline import Pipeline as ImPineline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.grid_search import RandomizedSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from scipy.signal import lfilter\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "import time\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spec = total_train_feature.iloc[:,:-1].head()\n",
    "\n",
    "def specLineNum(spec, windowSize=2, threshold=0.8):\n",
    "    fs = pd.Series()\n",
    "    ss = pd.Series()\n",
    "    for i in range(windowSize, spec.shape[1]):\n",
    "        s = spec.iloc[:,i-windowSize]\n",
    "        e = spec.iloc[:,i]\n",
    "        isFs = (e>threshold) & ((e-s)>threshold)\n",
    "        isSS = (e<threshold) & ((e-s)<-threshold)\n",
    "        if(fs.size==0):\n",
    "            fs = isFs.astype(int)\n",
    "        else:\n",
    "            fs = fs + isFs.astype(int)\n",
    "        if(ss.size==0):\n",
    "            ss = isSS.astype(int)\n",
    "        else:\n",
    "            ss = ss + isSS.astype(int)\n",
    "    combine = pd.concat([fs, ss], axis=1)\n",
    "    combine.columns = ['fs','ss']\n",
    "    return combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qsoFeature(data):\n",
    "    std1 = data.iloc[:,850:880].std(axis=1)\n",
    "    std2 = data.iloc[:,885:898].std(axis=1)\n",
    "    std3 = data.iloc[:,900:1000].std(axis=1)\n",
    "    f1 = (std1<0.3) & (std2>0.35) & (std3<0.35)\n",
    "    f3 = (std2*std2)/(std1*std3+0.1)\n",
    "    d1 = std1+0.2\n",
    "    f2 = (std2>d1)\n",
    "    qsof = pd.concat([f1, f2, f3], axis=1)\n",
    "    qsof.columns = ['qso_f1','qso_f2','qso_f3']\n",
    "    return (qsof)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intervalEWFeature(d, start=0, end=-1, filterThrehold=1):\n",
    "    cp = d\n",
    "#     cp[abs(cp)<filterThrehold] = 0\n",
    "    rd = cp.iloc[:, start:end]\n",
    "    specLines = specLineNum(rd[abs(rd)>filterThrehold])\n",
    "    ew_absorb = rd[(rd<0)&(abs(rd)>filterThrehold)].sum(axis=1)\n",
    "    ew_fs = rd[(rd>0)&(abs(rd)>filterThrehold)].sum(axis=1)\n",
    "    return pd.concat([ew_absorb, ew_fs, specLines], axis=1)\n",
    "\n",
    "def smooth(filterSet):\n",
    "    n = 5  # the larger n is, the smoother curve will be\n",
    "    b = [1.0 / n] * n\n",
    "    a = 1\n",
    "    filterSet = lfilter(b, a, filterSet, axis=1)\n",
    "    return pd.DataFrame(filterSet)\n",
    "\n",
    "def EWByRangeList(d, rangeList, filterThrehold=1, isSmooth=False, doScale=True):\n",
    "    if doScale:\n",
    "        d = pd.DataFrame(scale(d, axis=1))\n",
    "    if isSmooth:\n",
    "        d=smooth(d)\n",
    "    ewfAll = pd.DataFrame()\n",
    "    for r in rangeList:\n",
    "        start, end = r\n",
    "        ewf = intervalEWFeature(d, start, end, filterThrehold)\n",
    "        newNames = ['ew_' + str(start) + '_' + str(end) + '_' + str(v) for v in ewf.columns.values]\n",
    "        ewf.columns = newNames\n",
    "        ewfAll = pd.concat([ewfAll,ewf], axis=1)\n",
    "    return ewfAll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "def submit(fileName, testTotal, preds):\n",
    "    submission = testTotal[['id']]\n",
    "    submission['pred'] = preds\n",
    "    submission.to_csv('./predicts/'+fileName, header=False, index=False)\n",
    "    return submission\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractFeature(train_feature, partition=2, randomPartion=False, prefix=\"p\", onlyAvg=False, \n",
    "                   filterLow=False, levelFeature=False, scaleSpec=False, smooth=False,\n",
    "                   generalFeature = True,\n",
    "                  ):\n",
    "    if scaleSpec:\n",
    "        train_feature = pd.DataFrame(scale(train_feature, axis=1))\n",
    "    if smooth:\n",
    "        train_feature = train_feature.rolling(100, axis=1).mean()\n",
    "    if filterLow:\n",
    "        train_feature[train_feature<1] = 0\n",
    "    plen = train_feature.shape[1]/partition\n",
    "    features = pd.DataFrame()\n",
    "    avgs = pd.DataFrame()\n",
    "    for i in range(0, partition):\n",
    "        pstart = random.randint(0, train_feature.shape[1]-plen-1)\n",
    "        pendExclue = pstart + plen + 1\n",
    "        avgC = train_feature.iloc[:, pstart:pendExclue].mean(axis=1)\n",
    "        stdC = train_feature.iloc[:, pstart:pendExclue].std(axis=1)\n",
    "        maxC = train_feature.iloc[:, pstart:pendExclue].max(axis=1)\n",
    "        minC = train_feature.iloc[:, pstart:pendExclue].min(axis=1)\n",
    "        medianC = train_feature.iloc[:, pstart:pendExclue].median(axis=1)\n",
    "        diffC = train_feature.iloc[:, pstart:pendExclue].diff(axis=1).iloc[:,1:]\n",
    "        features[prefix+'_avg'+str(i)] = avgC\n",
    "        avgs[prefix+'_avg'+str(i)] = avgC\n",
    "        if onlyAvg==False:\n",
    "            features[prefix+'_std'+str(i)] = stdC\n",
    "            features[prefix+'_median'+str(i)] = medianC\n",
    "            features[prefix+'_max'+str(i)] = maxC\n",
    "            features[prefix+'_min'+str(i)] = minC\n",
    "            features[prefix+'_avg_m_std'+str(i)] = stdC/avgC\n",
    "            features[prefix+'_max_m_avg'+str(i)] = maxC/avgC\n",
    "            features[prefix+'_max_m_median'+str(i)] = maxC/(medianC+0.1)\n",
    "#             features[prefix+'_median_m_mean'+str(i)] = medianC/avgC\n",
    "#             features[prefix+'_min_m_median'+str(i)] = minC/(medianC+0.1)\n",
    "            features[prefix+'_min_m_avg'+str(i)] = minC/avgC\n",
    "#             features[prefix+'_media_m_avg'+str(i)] = medianC/avgC\n",
    "    if levelFeature:\n",
    "        level_feature = extractFeature(avgs, prefix=\"l\", partition = 4)\n",
    "        features = pd.concat([features, level_feature], axis = 1)\n",
    "    if generalFeature:\n",
    "        avgC = train_feature.mean(axis=1)\n",
    "        stdC = train_feature.std(axis=1)\n",
    "        maxC = train_feature.max(axis=1)\n",
    "        minC = train_feature.min(axis=1)\n",
    "        medianC = train_feature.median(axis=1)\n",
    "        features[prefix+'_std'] = stdC\n",
    "        features[prefix+'_median'] = medianC\n",
    "        features[prefix+'_max'] = maxC\n",
    "        features[prefix+'_min'] = minC\n",
    "        features[prefix+'_avg_m_std'] = stdC/avgC\n",
    "        features[prefix+'_max_m_avg'] = maxC/avgC\n",
    "        features[prefix+'_max_m_median'] = maxC/(medianC+0.1)\n",
    "        features[prefix+'_median_m_mean'] = medianC/avgC\n",
    "        features[prefix+'_min_m_median'] = minC/(medianC+0.1)\n",
    "        features[prefix+'_min_m_avg'] = minC/avgC\n",
    "    return features.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readTotalFeature(fileName, maxFileIndex, hasLabel=True, partition = 20, ewFeature=True, addQsoFeature=False, randEWNum=100):\n",
    "    total_train_feature = pd.DataFrame()\n",
    "    rangeList = [(0,300), (0, 500), (500, 1000),(850, 1000), (1000, 1300),(1500,2000), (1500, 2600), (2500, 2600)]\n",
    "    for i in range(0, maxFileIndex+1):\n",
    "        print('processing:' + str(i))\n",
    "        train_set = pd.read_csv(fileName + str(i) +'.csv', header=None)\n",
    "        renameCol = {0:'id'}\n",
    "        if hasLabel:\n",
    "            renameCol[train_set.shape[1]-1] = 'label'\n",
    "        train_set.rename(columns=renameCol, inplace=True)\n",
    "        if hasLabel:\n",
    "            processed_train = pd.DataFrame()\n",
    "            rawFeature = train_set.iloc[:,1:-1]\n",
    "        else:\n",
    "            rawFeature = train_set.iloc[:,1:]\n",
    "        processed_train = extractFeature(rawFeature, partition, levelFeature=False, smooth=False)\n",
    "        if ewFeature:\n",
    "            ew_feature = EWByRangeList(rawFeature, rangeList, filterThrehold=0.5, doScale=True)\n",
    "            processed_train = pd.concat([processed_train, ew_feature], axis=1)\n",
    "        if addQsoFeature:\n",
    "            qso_feature = qsoFeature(rawFeature)\n",
    "            processed_train = pd.concat([processed_train, qso_feature], axis=1)\n",
    "        if hasLabel:\n",
    "            processed_train['label'] = train_set.label\n",
    "        else:\n",
    "            processed_train['id'] = train_set.id\n",
    "        total_train_feature = total_train_feature.append(processed_train, ignore_index=True)\n",
    "    return total_train_feature.sample(total_train_feature.shape[0], replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "validMode = False\n",
    "reFeature = True\n",
    "partition = 45\n",
    "rng = np.random.RandomState(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing:0\n",
      "processing:1\n",
      "processing:2\n",
      "processing:3\n",
      "processing:4\n",
      "processing:5\n",
      "processing:6\n",
      "processing:7\n",
      "processing:8\n",
      "processing:9\n",
      "processing:10\n",
      "processing:11\n",
      "processing:12\n",
      "processing:13\n",
      "processing:14\n",
      "processing:15\n",
      "processing:16\n",
      "processing:17\n",
      "processing:18\n",
      "processing:19\n",
      "processing:20\n",
      "processing:21\n",
      "processing:22\n",
      "processing:23\n",
      "processing:24\n",
      "processing:25\n",
      "processing:26\n",
      "processing:27\n",
      "processing:28\n",
      "processing:29\n",
      "processing:30\n",
      "processing:31\n",
      "processing:32\n",
      "processing:33\n",
      "processing:34\n",
      "processing:35\n",
      "processing:36\n",
      "processing:37\n",
      "processing:38\n",
      "processing:39\n",
      "processing:40\n",
      "processing:41\n",
      "processing:42\n",
      "processing:43\n",
      "processing:44\n",
      "processing:45\n",
      "processing:46\n",
      "processing:47\n",
      "processing:48\n"
     ]
    }
   ],
   "source": [
    "if reFeature:\n",
    "    total_train_feature = readTotalFeature('./train_feature_all_', 48, partition=partition)\n",
    "    total_train_feature.to_csv('./train_feature_check_point.csv', index=False)\n",
    "else :\n",
    "    total_train_feature = pd.read_csv('./train_feature_check_point.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_train_feature = total_train_feature.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34288, 448)"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_train_feature[total_train_feature.label=='unknown'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(338695, 447)"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = total_train_feature.iloc[:,:-1]\n",
    "Y = total_train_feature.iloc[:,-1]\n",
    "\n",
    "# Split the dataset in two equal parts\n",
    "x, t_x, y, t_y = train_test_split(\n",
    "    X, Y, test_size=0.3, random_state=0)\n",
    "x.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outlier_rejection(X, y):\n",
    "    model = IsolationForest(n_jobs=-1, random_state=rng)\n",
    "    model.fit(X)\n",
    "    y_pred = model.predict(X)\n",
    "    return X[y_pred == 1], y[y_pred == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote = SMOTE()\n",
    "anova_filter = SelectKBest(f_classif, k=(int)(x.shape[1]*0.9))\n",
    "model = RandomForestClassifier(n_jobs=4, class_weight='balanced', max_depth=60, n_estimators=10, bootstrap=False, verbose=1,random_state=rng)\n",
    "# model = AdaBoostClassifier(random_state=rng)\n",
    "# model = GradientBoostingClassifier(random_state=rng,verbose=1)\n",
    "# model = XGBClassifier()\n",
    "# model = KNeighborsClassifier(n_neighbors = 3, n_jobs=-1)\n",
    "scaler = StandardScaler()\n",
    "scaler = Normalizer()\n",
    "poly = PolynomialFeatures(2)\n",
    "# scaler = RobustScaler()\n",
    "pca = PCA(n_components=50, svd_solver='full')\n",
    "ref = RFE(estimator=LogisticRegression(), n_features_to_select=50)\n",
    "# model = GaussianNB()\n",
    "clf = ImPineline([\n",
    "#     ('feture_select', anova_filter),\n",
    "#     ('pca',pca),\n",
    "#     ('ref', ref),\n",
    "    ('scaler',scaler),\n",
    "    ('smote',smote),\n",
    "    ('model', model)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "if validMode:\n",
    "#     rs = clf.fit(x, y)\n",
    "    scores = cross_val_score(clf, x, y, cv=3, n_jobs=-1, verbose=1, scoring='f1_macro')\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "#     preds_test = clf.predict(t_x)\n",
    "    # clf.score(t_x, t_y)\n",
    "#     print(\"%0.3f\" % f1_score(t_y, preds_test, average='macro'))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "if validMode:\n",
    "    rs = clf.fit(x, y)\n",
    "    preds_test = clf.predict(t_x)\n",
    "#     preds_test = carbateQso(t_x, preds_test)\n",
    "#     clf.score(t_x, t_y)\n",
    "    print(\"%0.3f\" % f1_score(t_y, preds_test, average='macro'))  \n",
    "    print(classification_report(t_y, preds_test, labels=['galaxy', 'qso', 'star', 'unknown']))\n",
    "    print(confusion_matrix(t_y, preds_test, labels=['galaxy', 'qso', 'star', 'unknown']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "if validMode:\n",
    "    clf = RandomForestClassifier(n_jobs=-1, class_weight='balanced',  verbose=1 )\n",
    "    rs = clf.fit(x, y)\n",
    "    preds_test = clf.predict(t_x)\n",
    "    # clf.score(t_x, t_y)\n",
    "    print('f1: %.2f' % f1_score(t_y, preds_test, average='macro')) \n",
    "    print(classification_report(t_y, preds_test, labels=['galaxy', 'qso', 'star', 'unknown']))\n",
    "    print(confusion_matrix(t_y, preds_test, labels=['galaxy', 'qso', 'star', 'unknown']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "if validMode:\n",
    "    importances = model.feature_importances_\n",
    "    std = np.std([tree.feature_importances_ for tree in model.estimators_],\n",
    "                 axis=0)\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "\n",
    "    # Print the feature ranking\n",
    "    print(\"Feature ranking:\")\n",
    "\n",
    "    for f in range(X.shape[1]):\n",
    "        print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n",
    "\n",
    "    # Plot the feature importances of the forest\n",
    "    plt.figure()\n",
    "    plt.title(\"Feature importances\")\n",
    "    plt.bar(range(X.shape[1]), importances[indices],\n",
    "           color=\"r\", yerr=std[indices], align=\"center\")\n",
    "    plt.xticks(range(X.shape[1]), indices)\n",
    "    plt.xlim([-1, X.shape[1]])\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "if validMode:\n",
    "    smote = SMOTE()\n",
    "    parameters = {\n",
    "        'max_depth':[60, 70, 80],  #70\n",
    "        'max_features':[0.5,0.6,0.7], #0.6\n",
    "    #     'min_samples_split':[2, 4, 8, 20], #2\n",
    "    #     'min_samples_leaf':[1,4,8,20], #1\n",
    "    #     'min_weight_fraction_leaf':[0,0.05,0.1,0.2], #0\n",
    "        'max_leaf_nodes':[800, 1000, 1200], #1000\n",
    "    #      'bootstrap':[True, False], #False\n",
    "    #     'oob_score':[True, False], #False\n",
    "    #     'class_weight':[None,'balanced'], #balanced\n",
    "        'n_estimators':[10, 20], #10\n",
    "    #     'criterion':['gini','entropy']\n",
    "    }\n",
    "    clf = GridSearchCV(RandomForestClassifier(n_jobs=4, max_depth=20), parameters, cv=5,\n",
    "                           scoring='f1_macro', n_jobs=8, verbose=2)\n",
    "    clf.fit(x, y)\n",
    "    print(clf.best_params_)\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing:0\n",
      "processing:1\n",
      "processing:2\n",
      "processing:3\n",
      "processing:4\n",
      "processing:5\n",
      "processing:6\n",
      "processing:7\n",
      "processing:8\n",
      "processing:9\n"
     ]
    }
   ],
   "source": [
    "if reFeature:\n",
    "    testTotal = readTotalFeature('./test_feature_all_', 9, hasLabel=False, partition=partition)\n",
    "    testTotal.to_csv('./test_feature_checkpoint.csv', index=False)\n",
    "    testTotal.head()\n",
    "else :\n",
    "    testTotal = pd.read_csv('./test_feature_checkpoint.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 448)"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testTotal.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed: 11.1min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('scaler', Normalizer(copy=True, norm='l2')), ('smote', SMOTE(k=None, k_neighbors=5, kind='regular', m=None, m_neighbors=10, n_jobs=1,\n",
       "   out_step=0.5, random_state=None, ratio='auto', svm_estimator=None)), ('model', RandomForestClassifier(bootstrap=False, class_weight='balanced',\n",
       "           ...random_state=<mtrand.RandomState object at 0x1c18d2c5a0>,\n",
       "            verbose=1, warm_start=False))])"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X, Y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    0.2s finished\n",
      "/Users/cql/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "test_x = testTotal.iloc[:,:-1]\n",
    "pred = clf.predict(test_x)\n",
    "timestr = time.strftime(\"%m%d_%H%M\")\n",
    "rs = submit(timestr+'_rf_ew_e100_pca_40.csv',testTotal, pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs.pred.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_train_feature.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_train_feature[:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
